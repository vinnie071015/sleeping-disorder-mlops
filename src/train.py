import os
import sys
import argparse
import datetime
import traceback
import subprocess
import time

# ==========================================
# 0. æ ¸å¿ƒé…ç½®ä¸æ—¥å¿—ç»„ä»¶
# ==========================================

# âš ï¸ è¯·ç¡®è®¤ä½ çš„ S3 æ¡¶åç§°
LOG_BUCKET_NAME = 'sleep-disorder-mlops-bucket' 
LOG_FILE_PATH = "/tmp/captured_log.txt"

class DualLogger:
    """
    æ‹¦æˆª sys.stdout å’Œ sys.stderrï¼Œ
    å°†å†…å®¹åŒæ—¶è¾“å‡ºåˆ°ï¼š
    1. æ§åˆ¶å° (CloudWatch)
    2. æœ¬åœ°æ–‡ä»¶ (/tmp/log.txt) -> ç”¨äºä¸Šä¼  S3
    """
    def __init__(self, original_stream, log_file_path):
        self.terminal = original_stream
        self.log_file_path = log_file_path
        # åˆå§‹åŒ–æ—¶ï¼Œå¦‚æœæ˜¯ stdout åˆ™ä¸éœ€è¦æ¸…ç©ºï¼ˆé¿å…åŒé‡æ¸…ç©ºï¼‰ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼šè¿½åŠ æ¨¡å¼
    
    def write(self, message):
        # 1. ç…§å¸¸æ‰“å°åˆ°æ§åˆ¶å°
        self.terminal.write(message)
        # 2. è¿½åŠ å†™å…¥æ–‡ä»¶
        try:
            with open(self.log_file_path, "a", encoding='utf-8') as f:
                f.write(message)
        except Exception:
            pass 

    def flush(self):
        self.terminal.flush()

def upload_logs_to_s3(local_path, bucket_name):
    """å°è¯•å°†æ—¥å¿—æ–‡ä»¶ä¸Šä¼ åˆ° S3"""
    try:
        import boto3 # å»¶è¿Ÿå¯¼å…¥ï¼Œç¡®ä¿ boto3 å¯ç”¨
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        s3_key = f"debug_logs/train_failure_log_{timestamp}.txt"
        
        # ä½¿ç”¨åŸç”Ÿ stdout æ‰“å°ï¼Œé˜²æ­¢é€’å½’æ­»å¾ªç¯
        sys.__stdout__.write(f"\n[S3 Upload] Uploading logs to s3://{bucket_name}/{s3_key} ...\n")
        
        s3 = boto3.client('s3')
        s3.upload_file(local_path, bucket_name, s3_key)
        
        sys.__stdout__.write(f"âœ… [S3 Upload] Success! Log saved to S3.\n")
    except Exception as e:
        sys.__stdout__.write(f"âŒ [S3 Upload] Failed: {e}\n")

# ==========================================
# 1. ä¾èµ–å®‰è£… (æ ¸å¼¹çº§æ¸…ç†æ¨¡å¼)
# ==========================================
def install_dependencies():
    print("\nğŸ“¦ [INIT] Start environment cleanup & installation...", flush=True)
    
    # --- 1. å…ˆæŠŠå®¹å™¨è‡ªå¸¦çš„åº“å…¨éƒ¨å¸è½½ (æ ¸å¼¹æ¸…ç†) ---
    # è¿™èƒ½é¿å… "Old Numpy" å’Œ "New Pandas" æ‰“æ¶
    troublemakers = ["numpy", "pandas", "scikit-learn", "joblib"]
    print(f"   aaa... Purging pre-installed libraries: {troublemakers}...", flush=True)
    try:
        # -y è¡¨ç¤ºè‡ªåŠ¨ç¡®è®¤ï¼Œé˜²æ­¢å¡ä½
        subprocess.check_call([sys.executable, "-m", "pip", "uninstall", "-y"] + troublemakers)
        print("   âœ… Cleanup complete.", flush=True)
    except Exception as e:
        # å¦‚æœå¸è½½å¤±è´¥ï¼ˆæ¯”å¦‚æœ¬æ¥å°±æ²¡è£…ï¼‰ï¼Œä¸è¦æŠ¥é”™ï¼Œç»§ç»­å¾€ä¸‹èµ°
        print(f"   âš ï¸ Cleanup warning (non-fatal): {e}", flush=True)

    # --- 2. å‡çº§ pip ---
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "pip"])
    except:
        pass

    # --- 3. é‡æ–°å®‰è£…æŒ‡å®šç‰ˆæœ¬ ---
    # æ—¢ç„¶å·²ç»å¸è½½äº†ï¼Œè¿™é‡Œå°±æ˜¯å…¨æ–°å®‰è£…ï¼Œä¸ä¼šæœ‰äºŒè¿›åˆ¶å†²çª
    packages = [
        "numpy==1.26.4",  # æŒ‡å®šä¸€ä¸ªç¨³å®šçš„æ–°ç‰ˆæœ¬
        "pandas==2.2.0",
        "scikit-learn==1.4.0",
        "matplotlib",
        "seaborn",
        "joblib",
        "wandb"
    ]
    
    for package in packages:
        try:
            cmd = [sys.executable, "-m", "pip", "install", package]
            print(f"   - Installing new: {package} ...", flush=True)
            subprocess.check_call(cmd)
        except Exception as e:
            print(f"   âš ï¸ Warning: Failed to install {package}. Error: {e}", flush=True)
            
    print("âœ… [INIT] Fresh dependencies installed.\n", flush=True)

# ==========================================
# 2. è®­ç»ƒé€»è¾‘ (å°è£…åœ¨å‡½æ•°ä¸­ï¼Œé¿å…å…¨å±€å¯¼å…¥æŠ¥é”™)
# ==========================================
def perform_training(args):
    print("ğŸ”„ [IMPORT] Loading ML libraries...", flush=True)
    
    # --- è¿™é‡Œçš„ Import å¿…é¡»æ”¾åœ¨å‡½æ•°å†…éƒ¨ ---
    # å› ä¸ºåœ¨ main() è¿è¡Œ install_dependencies() ä¹‹å‰ï¼Œè¿™äº›åŒ…å¯èƒ½ä¸å­˜åœ¨
    import joblib
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.metrics import accuracy_score
    
    # åŠ¨æ€å¯¼å…¥ src
    sys.path.append(os.getcwd())
    try:
        from src.data_processor import load_data, clean_data
        print("âœ… [IMPORT] src.data_processor loaded.", flush=True)
    except ImportError as e:
        print(f"âŒ [IMPORT] Failed to import src.data_processor: {e}", flush=True)
        # ç»§ç»­å°è¯•è¿è¡Œï¼Œæ–¹ä¾¿æ’æŸ¥è·¯å¾„é—®é¢˜
    
    # --------------------------------------------------------
    # Helper Functions (å†…éƒ¨å®šä¹‰)
    # --------------------------------------------------------
    def get_model(model_args):
        if model_args.model_type == 'lr': return LogisticRegression(C=model_args.C)
        elif model_args.model_type == 'svm': return SVC(C=model_args.C, kernel=model_args.kernel)
        elif model_args.model_type == 'rf': return RandomForestClassifier(n_estimators=model_args.n_estimators)
        else: raise ValueError(f"Unknown model type: {model_args.model_type}")

    def create_pipeline(cat_cols, num_cols, m_args):
        preprocessor = ColumnTransformer(transformers=[
            ('num', StandardScaler(), num_cols),
            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
        ])
        model = get_model(m_args)
        return Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])

    # --------------------------------------------------------
    # ä¸šåŠ¡é€»è¾‘ Start
    # --------------------------------------------------------
    print("\n--- 1. Data Loading ---", flush=True)
    
    # [FIX 3] å†æ¬¡ç¡®è®¤è·¯å¾„ï¼Œç¡®ä¿éç©º
    data_dir = args.train
    print(f"DATA_DIAG: Target data directory: {data_dir}", flush=True)

    if not data_dir:
        raise ValueError("âŒ Error: args.train is Empty! Check argparse defaults.")

    file_path = os.path.join(data_dir, "sleep_data.csv")
    print(f"DATA_DIAG: Full file path: {file_path}", flush=True)

    if not os.path.exists(file_path):
        print(f"âŒ [ERROR] File not found at {file_path}. Listing dir contents:", flush=True)
        if os.path.exists(data_dir):
            print(os.listdir(data_dir), flush=True)
        else:
            print(f"   Directory {data_dir} does not exist!", flush=True)
        raise FileNotFoundError(f"Data file missing: {file_path}")

    df = load_data(file_path)
    df = clean_data(df)
    print(f"DATA_DIAG: Data Loaded. Shape: {df.shape}", flush=True)

    # ç‰¹å¾å¤„ç†
    target_col = 'sleep_disorder'
    if target_col not in df.columns:
        raise ValueError(f"Target {target_col} missing.")

    df[target_col] = df[target_col].fillna('None')
    le = LabelEncoder()
    df[target_col] = le.fit_transform(df[target_col])
    
    X = df.drop(columns=[target_col, 'person_id'], errors='ignore')
    y = df[target_col]
    
    # è®­ç»ƒ
    print("\n--- 2. Training ---", flush=True)
    cat_features = X.select_dtypes(include=['object']).columns
    num_features = X.select_dtypes(include=['number']).columns
    
    pipeline = create_pipeline(cat_features, num_features, args)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    pipeline.fit(X_train, y_train)
    print("STATUS: Model fitting completed.", flush=True)

    # è¯„ä¼°ä¸ä¿å­˜
    print("\n--- 3. Evaluation & Saving ---", flush=True)
    acc = accuracy_score(y_test, pipeline.predict(X_test))
    print(f"âœ… Accuracy: {acc:.4f}", flush=True) # ä¿®æ”¹æ ¼å¼ä»¥åŒ¹é… Metric Regex

    # ä¿å­˜
    if not os.path.exists(args.model_dir):
        os.makedirs(args.model_dir)
        
    joblib.dump(pipeline, os.path.join(args.model_dir, "model.joblib"))
    joblib.dump(le, os.path.join(args.model_dir, "label_encoder.joblib"))
    print(f"âœ… FINAL: Model saved to {args.model_dir}", flush=True)


# ==========================================
# 3. ä¸»å…¥å£ (åŒ…å«æ—¥å¿—åŠ«æŒ)
# ==========================================
if __name__ == '__main__':
    # 1. åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
    with open(LOG_FILE_PATH, "w", encoding='utf-8') as f:
        f.write(f"=== TRAINING SESSION STARTED: {datetime.datetime.now()} ===\n")

    # 2. åŠ«æŒè¾“å‡º
    sys.stdout = DualLogger(sys.stdout, LOG_FILE_PATH)
    sys.stderr = DualLogger(sys.stderr, LOG_FILE_PATH)

    print("--- ğŸš€ SCRIPT START ---", flush=True)
    
    try:
        # 3. è§£æå‚æ•°
        parser = argparse.ArgumentParser()
        parser.add_argument('--model_type', type=str, default='svm')
        parser.add_argument('--n_estimators', type=int, default=100)
        parser.add_argument('--C', type=float, default=1.0)
        parser.add_argument('--kernel', type=str, default='rbf')
        
        # [FIX 4] å¢å¼ºçš„è·¯å¾„å¤„ç†é€»è¾‘
        # ä¼˜å…ˆè¯»å–ç¯å¢ƒå˜é‡ï¼Œå¦‚æœæ²¡æœ‰ï¼Œé»˜è®¤ä¸º '/opt/ml/input/data/train'
        env_sm_channel = os.environ.get('SM_CHANNEL_TRAINING')
        default_data_path = env_sm_channel if env_sm_channel else '/opt/ml/input/data/train'
        
        parser.add_argument('--train', type=str, default=default_data_path)
        parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', '/opt/ml/model'))
        
        args, _ = parser.parse_known_args() # ä½¿ç”¨ parse_known_args å®¹é”™æ€§æ›´å¥½

        print(f"INFO: Arguments: {args}", flush=True)
        print(f"INFO: Env SM_CHANNEL_TRAINING: {env_sm_channel}", flush=True)
        print(f"INFO: Effective Data Path: {args.train}", flush=True)

        # 4. æ‰§è¡Œå®‰è£…å’Œè®­ç»ƒ
        install_dependencies()
        perform_training(args)

    except Exception:
        # 5. æ•è·ä¸€åˆ‡å´©æºƒ
        print("\nâŒ CRASH DETECTED! Printing Traceback:", flush=True)
        traceback.print_exc()
        # æ­¤æ—¶ sys.stderr ä¹Ÿæ˜¯ DualLoggerï¼Œæ‰€ä»¥ traceback ä¹Ÿä¼šå†™å…¥æ–‡ä»¶
        
    finally:
        # 6. æœ€ç»ˆä¸Šä¼ æ—¥å¿—
        print("\n--- ğŸ SCRIPT FINISHING ---", flush=True)
        print("INFO: Initiating log upload procedure...", flush=True)
        
        # æ¢å¤æ ‡å‡†è¾“å‡ºï¼Œç¡®ä¿ boto3 ä¸å—å¹²æ‰°
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__
        
        upload_logs_to_s3(LOG_FILE_PATH, LOG_BUCKET_NAME)
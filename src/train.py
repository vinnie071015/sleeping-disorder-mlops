import os
import sys
import argparse
import datetime
import traceback
import subprocess
import time

# ==========================================
# 0. æ ¸å¿ƒé…ç½®ä¸æ—¥å¿—ç»„ä»¶
# ==========================================

# âš ï¸ è¯·ç¡®è®¤ä½ çš„ S3 æ¡¶åç§°
LOG_BUCKET_NAME = 'sleep-disorder-mlops-bucket' 
LOG_FILE_PATH = "/tmp/captured_log.txt"

class DualLogger:
    """
    æ‹¦æˆª sys.stdout å’Œ sys.stderrï¼Œ
    å°†å†…å®¹åŒæ—¶è¾“å‡ºåˆ°ï¼š
    1. æ§åˆ¶å° (CloudWatch)
    2. æœ¬åœ°æ–‡ä»¶ (/tmp/log.txt) -> ç”¨äºä¸Šä¼  S3
    """
    def __init__(self, original_stream, log_file_path):
        self.terminal = original_stream
        self.log_file_path = log_file_path
        # åˆå§‹åŒ–æ—¶ï¼Œå¦‚æœæ˜¯ stdout åˆ™ä¸éœ€è¦æ¸…ç©ºï¼ˆé¿å…åŒé‡æ¸…ç©ºï¼‰ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼šè¿½åŠ æ¨¡å¼
        # å®é™…ç”±å¤–éƒ¨æ§åˆ¶æ–‡ä»¶åˆå§‹åŒ–
        
    def write(self, message):
        # 1. ç…§å¸¸æ‰“å°åˆ°æ§åˆ¶å°
        self.terminal.write(message)
        # 2. è¿½åŠ å†™å…¥æ–‡ä»¶
        try:
            with open(self.log_file_path, "a", encoding='utf-8') as f:
                f.write(message)
        except Exception:
            pass 

    def flush(self):
        self.terminal.flush()

def upload_logs_to_s3(local_path, bucket_name):
    """å°è¯•å°†æ—¥å¿—æ–‡ä»¶ä¸Šä¼ åˆ° S3"""
    try:
        import boto3 # å»¶è¿Ÿå¯¼å…¥ï¼Œç¡®ä¿ boto3 å¯ç”¨
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        s3_key = f"debug_logs/train_failure_log_{timestamp}.txt"
        
        # ä½¿ç”¨åŸç”Ÿ stdout æ‰“å°ï¼Œé˜²æ­¢é€’å½’æ­»å¾ªç¯
        sys.__stdout__.write(f"\n[S3 Upload] Uploading logs to s3://{bucket_name}/{s3_key} ...\n")
        
        s3 = boto3.client('s3')
        s3.upload_file(local_path, bucket_name, s3_key)
        
        sys.__stdout__.write(f"âœ… [S3 Upload] Success! Log saved to S3.\n")
    except Exception as e:
        sys.__stdout__.write(f"âŒ [S3 Upload] Failed: {e}\n")

# ==========================================
# 1. ä¾èµ–å®‰è£… (åœ¨å¯¼å…¥ ML åº“ä¹‹å‰æ‰§è¡Œ)
# ==========================================
def install_dependencies():
    print("\nğŸ“¦ [INIT] Start installing dependencies...", flush=True)
    packages = [
        "pandas",
        "scikit-learn",
        "matplotlib",
        "seaborn",
        "joblib",
        "wandb"
    ]
    for package in packages:
        try:
            print(f"   - Installing {package}...", flush=True)
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        except Exception as e:
            print(f"   âš ï¸ Warning: Failed to install {package}. Error: {e}", flush=True)
    print("âœ… [INIT] Dependencies installed.\n", flush=True)

# ==========================================
# 2. è®­ç»ƒé€»è¾‘ (å°è£…åœ¨å‡½æ•°ä¸­ï¼Œé¿å…å…¨å±€å¯¼å…¥æŠ¥é”™)
# ==========================================
def perform_training(args):
    print("ğŸ”„ [IMPORT] Loading ML libraries...", flush=True)
    
    # --- è¿™é‡Œçš„ Import å¿…é¡»æ”¾åœ¨å‡½æ•°å†…éƒ¨ ---
    # å› ä¸ºåœ¨ main() è¿è¡Œ install_dependencies() ä¹‹å‰ï¼Œè¿™äº›åŒ…å¯èƒ½ä¸å­˜åœ¨
    import joblib
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.metrics import accuracy_score
    
    # åŠ¨æ€å¯¼å…¥ src
    sys.path.append(os.getcwd())
    try:
        from src.data_processor import load_data, clean_data
        print("âœ… [IMPORT] src.data_processor loaded.", flush=True)
    except ImportError as e:
        print(f"âŒ [IMPORT] Failed to import src.data_processor: {e}", flush=True)
        # ç»§ç»­å°è¯•è¿è¡Œï¼Œæˆ–è€…åœ¨è¿™é‡Œ raise
    
    # --------------------------------------------------------
    # Helper Functions (å†…éƒ¨å®šä¹‰)
    # --------------------------------------------------------
    def get_model(model_args):
        if model_args.model_type == 'lr': return LogisticRegression(C=model_args.C)
        elif model_args.model_type == 'svm': return SVC(C=model_args.C, kernel=model_args.kernel)
        elif model_args.model_type == 'rf': return RandomForestClassifier(n_estimators=model_args.n_estimators)
        else: raise ValueError(f"Unknown model type: {model_args.model_type}")

    def create_pipeline(cat_cols, num_cols, m_args):
        preprocessor = ColumnTransformer(transformers=[
            ('num', StandardScaler(), num_cols),
            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
        ])
        model = get_model(m_args)
        return Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])

    # --------------------------------------------------------
    # ä¸šåŠ¡é€»è¾‘ Start
    # --------------------------------------------------------
    print("\n--- 1. Data Loading ---", flush=True)
    
    # å¦‚æœæœ¬åœ°æµ‹è¯•æ²¡æœ‰ pathï¼Œæä¾›é»˜è®¤å€¼é˜²æ­¢æŠ¥é”™
    data_dir = args.train if args.train else "./data" 
    file_path = os.path.join(data_dir, "sleep_data.csv")
    print(f"DATA_DIAG: Loading from {file_path}", flush=True)

    if not os.path.exists(file_path):
        print(f"âŒ [ERROR] File not found at {file_path}. Listing dir:", flush=True)
        if os.path.exists(data_dir):
            print(os.listdir(data_dir), flush=True)
        raise FileNotFoundError(f"Data file missing: {file_path}")

    df = load_data(file_path)
    df = clean_data(df)
    print(f"DATA_DIAG: Data Loaded. Shape: {df.shape}", flush=True)

    # ç‰¹å¾å¤„ç†
    target_col = 'sleep_disorder'
    if target_col not in df.columns:
        raise ValueError(f"Target {target_col} missing.")

    df[target_col] = df[target_col].fillna('None')
    le = LabelEncoder()
    df[target_col] = le.fit_transform(df[target_col])
    
    X = df.drop(columns=[target_col, 'person_id'], errors='ignore')
    y = df[target_col]
    
    # è®­ç»ƒ
    print("\n--- 2. Training ---", flush=True)
    cat_features = X.select_dtypes(include=['object']).columns
    num_features = X.select_dtypes(include=['number']).columns
    
    pipeline = create_pipeline(cat_features, num_features, args)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    pipeline.fit(X_train, y_train)
    print("STATUS: Model fitting completed.", flush=True)

    # è¯„ä¼°ä¸ä¿å­˜
    print("\n--- 3. Evaluation & Saving ---", flush=True)
    acc = accuracy_score(y_test, pipeline.predict(X_test))
    print(f"RESULT: Accuracy: {acc:.4f}", flush=True)

    # ä¿å­˜
    if not os.path.exists(args.model_dir):
        os.makedirs(args.model_dir)
        
    joblib.dump(pipeline, os.path.join(args.model_dir, "model.joblib"))
    joblib.dump(le, os.path.join(args.model_dir, "label_encoder.joblib"))
    print(f"âœ… FINAL: Model saved to {args.model_dir}", flush=True)


# ==========================================
# 3. ä¸»å…¥å£ (åŒ…å«æ—¥å¿—åŠ«æŒ)
# ==========================================
if __name__ == '__main__':
    # 1. åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
    with open(LOG_FILE_PATH, "w", encoding='utf-8') as f:
        f.write(f"=== TRAINING SESSION STARTED: {datetime.datetime.now()} ===\n")

    # 2. åŠ«æŒè¾“å‡º
    sys.stdout = DualLogger(sys.stdout, LOG_FILE_PATH)
    sys.stderr = DualLogger(sys.stderr, LOG_FILE_PATH)

    print("--- ğŸš€ SCRIPT START ---", flush=True)
    
    try:
        # 3. è§£æå‚æ•°
        parser = argparse.ArgumentParser()
        parser.add_argument('--model_type', type=str, default='svm')
        parser.add_argument('--n_estimators', type=int, default=100)
        parser.add_argument('--C', type=float, default=1.0)
        parser.add_argument('--kernel', type=str, default='rbf')
        # SageMaker ç¯å¢ƒå˜é‡é»˜è®¤å€¼
        parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))
        parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', '/tmp/model'))
        
        args, _ = parser.parse_known_args() # ä½¿ç”¨ parse_known_args å®¹é”™æ€§æ›´å¥½

        print(f"INFO: Arguments: {args}", flush=True)
        print(f"INFO: Env SM_CHANNEL_TRAINING: {os.environ.get('SM_CHANNEL_TRAINING')}", flush=True)

        # 4. æ‰§è¡Œå®‰è£…å’Œè®­ç»ƒ
        install_dependencies()
        perform_training(args)

    except Exception:
        # 5. æ•è·ä¸€åˆ‡å´©æºƒ
        print("\nâŒ CRASH DETECTED! Printing Traceback:", flush=True)
        traceback.print_exc()
        # æ­¤æ—¶ sys.stderr ä¹Ÿæ˜¯ DualLoggerï¼Œæ‰€ä»¥ traceback ä¹Ÿä¼šå†™å…¥æ–‡ä»¶
        
    finally:
        # 6. æœ€ç»ˆä¸Šä¼ æ—¥å¿—
        print("\n--- ğŸ SCRIPT FINISHING ---", flush=True)
        print("INFO: Initiating log upload procedure...", flush=True)
        
        # æ¢å¤æ ‡å‡†è¾“å‡ºï¼Œç¡®ä¿ boto3 ä¸å—å¹²æ‰°
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__
        
        upload_logs_to_s3(LOG_FILE_PATH, LOG_BUCKET_NAME)